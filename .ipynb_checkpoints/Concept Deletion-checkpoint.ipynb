{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bec76ef-66ad-48a0-b240-11622fd64d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import pandas as pd\n",
    "import zstandard as zst\n",
    "import json\n",
    "import zstandard as zstd\n",
    "import matplotlib.pyplot as plt\n",
    "import time \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.multiprocessing as mp\n",
    "import torch.distributed as dist\n",
    "\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from copy import deepcopy\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import pipeline, Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa6f6143-ffd6-4d22-9b90-ca29b51e615b",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_BATCH_SIZE = 4  # 32 here increased training x5 times\n",
    "TEXT_LEN = 128\n",
    "LR = 1e-2\n",
    "L2 = 1e-8  # to regularize Adam with weight_decay\n",
    "MIN_LR = 1e-5\n",
    "LR_STEP_RATE = 1  # how many zst files to do lr rate decay\n",
    "LATENT_SIZE = 2048\n",
    "NUM_PROTOTYPES = 32000\n",
    "GEN_TEXT_LEN = 128\n",
    "TEXT_EVAL_GEN = ['Once upon a time,']\n",
    "QA_TEXT_EVAL_GEN = ['<|system|>\\nYou are a friendly chatbot who always responds in a helpful manner</s>\\n<|user|>\\nCan you give me some helpful financial advice?</s>\\n<|assistant|>\\n']\n",
    "DEVICE = 'cuda:2' if torch.cuda.is_available() else 'cpu'\n",
    "DIR = 'SlimPajama-627B'\n",
    "k=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cde5303-febf-468b-913d-294e13650c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training(plotting_acc, plotting_loss, plotting_topk_acc) -> None:\n",
    "\tfolder_name = 'plots/'\n",
    "\t\n",
    "\t# Plot running accuracy and top-k accuracy\n",
    "\tplt.figure(figsize=(10, 5))\n",
    "\tplt.plot(plotting_acc, label='Running Accuracy', color='b')\n",
    "\tplt.plot(plotting_topk_acc, label='Top-k Accuracy', color='g')\n",
    "\tplt.xlabel('Iterations')\n",
    "\tplt.ylabel('Accuracy')\n",
    "\tplt.title('Training Accuracy')\n",
    "\tplt.legend()\n",
    "\tacc_plot_path = os.path.join(folder_name, 'plotting_acc_and_topk.png')\n",
    "\tplt.savefig(acc_plot_path)\n",
    "\tplt.close()\n",
    "\t\n",
    "\t# Plot running loss\n",
    "\tplt.figure(figsize=(10, 5))\n",
    "\tplt.plot(plotting_loss, label='Running Loss', color='r')\n",
    "\tplt.xlabel('Iterations')\n",
    "\tplt.ylabel('Loss')\n",
    "\tplt.title('Training Loss')\n",
    "\tplt.legend()\n",
    "\tloss_plot_path = os.path.join(folder_name, 'plotting_loss.png')\n",
    "\tplt.savefig(loss_plot_path)\n",
    "\tplt.close()\n",
    "    \n",
    "\n",
    "def load_data(compressed_file_path) -> pd.DataFrame():\n",
    "\t\"\"\"\n",
    "\tDownloads a url from hugging face and returns a df\n",
    "\tMade because dataset is too large for initial tests\n",
    "\tCan probably remove this later when we get a large AWS and download the whole dataset\n",
    "\t\"\"\"\n",
    "\tdef read_jsonl_zst(file_path) -> None:\n",
    "\t\t\"\"\"\n",
    "\t\tExtracts jsonl into readable format for pandas\n",
    "\t\t\"\"\"\n",
    "\t\twith open(file_path, 'rb') as file:\n",
    "\t\t\tdecompressor = zst.ZstdDecompressor()\n",
    "\t\t\tstream_reader = decompressor.stream_reader(file)\n",
    "\t\t\tstream = io.TextIOWrapper(stream_reader, encoding = \"utf-8\")\n",
    "\t\t\tfor line in stream:\n",
    "\t\t\t\tyield json.loads(line)\n",
    "\t\t\t\n",
    "\tdata = list(read_jsonl_zst(compressed_file_path))\n",
    "\tdf = pd.DataFrame(data)\n",
    "\treturn df \n",
    "\t \n",
    "\n",
    "def generate_text(model, pwnet, tokenizer, max_new_tokens=128):\n",
    "\tpwnet.eval()\n",
    "\tfor prompt in [TEXT_EVAL_GEN, QA_TEXT_EVAL_GEN]:\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tinput_ids = tokenizer(prompt, padding=True, return_tensors=\"pt\").input_ids\n",
    "\t\t\tinput_ids = input_ids.to(model.device)\n",
    "\t\t\t\n",
    "\t\t\t# Generate tokens iteratively\n",
    "\t\t\tgenerated_ids = input_ids\n",
    "\t\t\tgenerated_pwnet_ids = input_ids.clone().detach()\n",
    "\t\t\tgenerate = True\n",
    "\t\t\tgenerate_pwnet = True\n",
    "\t\t\t\n",
    "\t\t\tfor _ in range(max_new_tokens):\n",
    "\t\t\t\t# Get model outputs and z\n",
    "\t\t\t\tz1 = model.model(generated_ids).last_hidden_state[0]\n",
    "\t\t\t\tz2 = model.model(generated_pwnet_ids).last_hidden_state[0]\n",
    "\t\t\t\tlogits = model.lm_head(z1)\n",
    "\t\t\t\tpwnet_logits = pwnet(z2)\n",
    "\n",
    "\t\t\t\t# Sample next token\n",
    "\t\t\t\tnext_token_id = torch.argmax(logits, dim=1)[-1].to(model.device)\n",
    "\t\t\t\tnext_pwnet_token_id = torch.argmax(pwnet_logits, dim=1)[-1].to(model.device)\n",
    "\n",
    "\t\t\t\t# Append the new token to the generated sequence\n",
    "\t\t\t\tif generate:\n",
    "\t\t\t\t\tgenerated_ids = torch.cat((generated_ids, next_token_id.view(1,1)), dim=-1)\n",
    "\t\t\t\tif generate_pwnet:\n",
    "\t\t\t\t\tgenerated_pwnet_ids = torch.cat((generated_pwnet_ids, next_pwnet_token_id.view(1,1)), dim=-1)\n",
    "\n",
    "\t\t\t\t# Stop if the end of sequence token is generated\n",
    "\t\t\t\tif next_token_id == tokenizer.eos_token_id:\n",
    "\t\t\t\t\tgenerate = False\n",
    "\t\t\t\tif next_pwnet_token_id == tokenizer.eos_token_id:\n",
    "\t\t\t\t\tgenerate_pwnet = False\n",
    "\t\t\t\t\t\n",
    "\t\t\t\tif not generate and not generate_pwnet:\n",
    "\t\t\t\t\tbreak\n",
    "\n",
    "\t\t\t# Decode the generated sequence\n",
    "\t\t\tgenerated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=False)\n",
    "\t\t\t# print('\\nBlack-box text: ============================')\n",
    "\t\t\t# print(tokenizer.decode(generated_ids[0], skip_special_tokens=False))\n",
    "\t\t\tprint('\\nPW-Net text ================================')\n",
    "\t\t\tprint(tokenizer.decode(generated_pwnet_ids[0], skip_special_tokens=False))\n",
    "\t\t\tprint(' ')\n",
    "\n",
    "\tpwnet.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e10d3318-04b4-4a27-98b4-c6e0938109d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_proto_llm(tiny_llama, tokenizer, pwnet, k=10):\n",
    "\n",
    "\tcriterion = nn.CrossEntropyLoss()  # Example loss function\n",
    "\tlr_decay_factor = 0.9  # Factor to decrease the learning rate\n",
    "\toptimizer = optim.Adam(pwnet.parameters(), lr=LR, weight_decay=L2)\n",
    "\tscaler = GradScaler()\n",
    "\n",
    "\t# Directory containing the CSV files\n",
    "\troot_data_dir = DIR + '/train/'\n",
    "\n",
    "\tplotting_acc = list()\n",
    "\tplotting_loss = list()\n",
    "\tplotting_topk_acc = list()\n",
    "\n",
    "\toptimizer.zero_grad()\n",
    "\n",
    "\tcount = 0\n",
    "\tfile_count = 0  # To keep track of the number of .zst files processed\n",
    "\tstart_time = time.time()\n",
    "\t\t\n",
    "\t# Iterate chunk directories\n",
    "\tfor sub_dir in os.listdir(root_data_dir):\n",
    "\t\tsub_dir_path = os.path.join(root_data_dir, sub_dir)\n",
    "\t\tif os.path.isdir(sub_dir_path):  # Check if it's a directory\n",
    "\t\t\t\n",
    "\t\t\t# Iterate .zst files in chunk directories\n",
    "\t\t\tfor zst_file in os.listdir(sub_dir_path):\n",
    "\t\t\t\tif zst_file.endswith('.zst'):\n",
    "\t\t\t\t\tfile_path = os.path.join(sub_dir_path, zst_file)\n",
    "\t\t\t\t\tdf = load_data(file_path)\n",
    "\t\t\t\t\ttext_data = df.text.values.tolist()\n",
    "\t\t\t\t\tnum_text_batches = len(text_data) // TEXT_BATCH_SIZE\n",
    "\n",
    "\t\t\t\t\tprint(\n",
    "\t\t\t\t\t\t\"DF Shape:\", df.shape, \n",
    "\t\t\t\t\t\t\"  --len(text data):\", len(text_data),\n",
    "\t\t\t\t\t\t\"  --num text batches:\", num_text_batches\n",
    "\t\t\t\t\t\t)\n",
    "\n",
    "\n",
    "\t\t\t\t\t# Here we start iterating the CSV in chunks\n",
    "\t\t\t\t\tfor text_batch_idx in range(num_text_batches):\n",
    "\t\t\t\t\t\ttext_batch_data = text_data[text_batch_idx * TEXT_BATCH_SIZE: (text_batch_idx+1) * TEXT_BATCH_SIZE]\n",
    "\n",
    "\t\t\t\t\t\twith torch.autocast(device_type='cuda'):\n",
    "\t\t\t\t\t\t\twith torch.no_grad():\n",
    "\t\t\t\t\t\t\t\tinput_ids = tokenizer(text_batch_data, max_length=TEXT_LEN, return_tensors=\"pt\", padding=True, truncation=True).input_ids\n",
    "\t\t\t\t\t\t\t\tinput_ids = input_ids.to(DEVICE)\n",
    "\t\t\t\t\t\t\t\tz = tiny_llama.model(input_ids).last_hidden_state\n",
    "\t\t\t\t\t\t\t\tbb_logits = tiny_llama.lm_head(z)\n",
    "\n",
    "\t\t\t\t\t\tz = z.view(-1, LATENT_SIZE)\n",
    "\t\t\t\t\t\tbb_logits = bb_logits.view(-1, NUM_CLASSES)\n",
    "\t\t\t\t\t\tlabels = torch.argmax(bb_logits, dim=1)\n",
    "\n",
    "\t\t\t\t\t\twith torch.autocast(device_type='cuda'):\n",
    "\t\t\t\t\t\t\tlogits = pwnet(z)\n",
    "\t\t\t\t\t\t\tloss = criterion(logits, labels.to(DEVICE))\n",
    "\n",
    "\t\t\t\t\t\tscaler.scale(loss).backward()\n",
    "\t\t\t\t\t\tscaler.step(optimizer)\n",
    "\t\t\t\t\t\tscaler.update()\n",
    "\t\t\t\t\t\toptimizer.zero_grad()\n",
    "\t\t\t\t\t\t\n",
    "\n",
    "\t\t\t\t\t\tacc = sum(torch.argmax(logits.cpu(), dim=1) == labels.cpu()) / len(labels.cpu())\n",
    "\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t# Calculate top-k accuracy\n",
    "\t\t\t\t\t\ttop_k = torch.topk(logits, k, dim=1).indices\n",
    "\t\t\t\t\t\ttopk_correct = (labels.cpu().unsqueeze(1).expand_as(top_k) == top_k.cpu()).any(dim=1).float().sum().item()\n",
    "\t\t\t\t\t\ttopk_acc = topk_correct / len(labels)\n",
    "\n",
    "\t\t\t\t\t\tplotting_loss.append(loss.item())\n",
    "\t\t\t\t\t\tplotting_acc.append(acc.item())\n",
    "\t\t\t\t\t\tplotting_topk_acc.append(topk_acc)\n",
    "\n",
    "\t\t\t\t\t\tcount += 1\n",
    "\n",
    "\t\t\t\t\t\tplot_training(plotting_acc, plotting_loss, plotting_topk_acc)\n",
    "\n",
    "\t\t\t\t\tfile_count += 1\n",
    "\n",
    "\t\t\t\t\t# Decrease learning rate every .zst files\n",
    "\t\t\t\t\tif file_count % LR_STEP_RATE == 0:\n",
    "\t\t\t\t\t\tfor param_group in optimizer.param_groups:\n",
    "\t\t\t\t\t\t\tnew_lr = param_group['lr'] * lr_decay_factor\n",
    "\t\t\t\t\t\t\tparam_group['lr'] = max(new_lr, MIN_LR)  # Ensure lr doesn't drop below MIN_LR\n",
    "\t\t\t\t\t\tprint(f\"Decreased learning rate to {optimizer.param_groups[0]['lr']} after processing {file_count} files\")\n",
    "\n",
    "\t\t\t\t\tprint(\n",
    "\t\t\t\t\t\t  '\\nRun Loss:', round(  sum(plotting_loss[-50:]) / len(plotting_loss[-50:])  , 2),\n",
    "\t\t\t\t\t\t  ' -- Acc:',      round(  sum(plotting_acc[-50:]) / len(plotting_acc[-50:])  , 2), \n",
    "\t\t\t\t\t\t  ' -- Top-'+str(k)+' Accuracy:', round(  sum(plotting_topk_acc[-50:]) / len(plotting_topk_acc[-50:])  , 2),\n",
    "\t\t\t\t\t\t  ' -- Iter:', count,\n",
    "\t\t\t\t\t\t  ' -- Dir:', file_path, \n",
    "\t\t\t\t\t\t  )\n",
    "\n",
    "\t\t\t\t\tprint(\"\\nTime Taken:\", time.time() - start_time)\n",
    "\n",
    "\t\t\t\t\tgenerate_text(tiny_llama, pwnet, tokenizer, max_new_tokens=GEN_TEXT_LEN)\n",
    "\n",
    "\t\t\t\t\ttorch.save(pwnet.module.state_dict(), 'weights/pwnet_'+str(count)+'.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d793879e-1b2b-4cd1-974d-8988270a8513",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "tiny_llama = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "pwnet = PWNet(LATENT_SIZE, PROTOTYPE_SIZE, NUM_PROTOTYPES, DEVICE)\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    pwnet = nn.DataParallel(pwnet)\n",
    "    pwnet.to(DEVICE)\n",
    "\n",
    "# Initial evaluation\n",
    "generate_text(tiny_llama, pwnet, tokenizer, max_new_tokens=GEN_TEXT_LEN)\t\n",
    "\n",
    "train_proto_llm(tiny_llama, tokenizer, pwnet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce419196-f66b-4450-a54d-505b7d5b01ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9731191f-6e11-49c0-9fdc-8e85d744480a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44586182-f681-4f56-8bb1-05d958e258ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "llm_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
